{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53108b6e",
   "metadata": {},
   "source": [
    "# --- Model Training ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55639cf5",
   "metadata": {},
   "source": [
    "Para esta sección vamos a hacer una función para cada modelo.\n",
    "\n",
    "Cada función de modelaje será hecha con GridSearchCV para la selección de hiperparámetros. Se usará TimeSeriesSplit para la validación cruzada. Cada función nos devolverá el modelo entrenado con el resultado de las metricas MAE, RMSE y MAPE. Nos entregará un diccionario con estas métricas. Cada función también va a imprimir estas métricas. Cada función va a tomar 6 parámetros. X_train, X_validation, X_test, y_train, y_validation, y_test.\n",
    "\n",
    "Habrá una función que divida el dataset de la manera que se estipula en el pdf. Siendo esta train 2017, validation 1er sem 2018, test 2do sem 2018.\n",
    "\n",
    "Habrá una `función maestra` que como parámetro solo tenga el df. Aquí se van a ejecutar todas las funciones. Primero dividiendo el dataset, y de ahí yendo en el siguiente orden: Regresiones (Lineal, Estocastica y Ridge), Modelos avanzados (Random Forest y Gradient Boosting (XGBoost, LightGBM y CatBoost)) y Ensamblador (stacking y blending). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c7ab0459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaciones de la librerias necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, StackingRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5d58b34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Semilla general para asegurar reproducibilidad\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "be3fa438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar dataset\n",
    "df = pd.read_csv('../data/processed/modeling_dataset.csv')\n",
    "df['fecha_primera_sesion'] = pd.to_datetime(df['fecha_primera_sesion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "17cf9d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# División del dataset -> 6 meses de 2017 y 2 trimestres de 2018\n",
    "def dividir_dataset(df, fecha_col='fecha_primera_sesion'):\n",
    "    df[fecha_col] = pd.to_datetime(df[fecha_col])\n",
    "    train = df[df[fecha_col].dt.year == 2017]\n",
    "    val = df[(df[fecha_col].dt.year == 2018) & (df[fecha_col].dt.month <= 3)]\n",
    "    test = df[(df[fecha_col].dt.year == 2018) & (df[fecha_col].dt.month > 3)]\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "51f5a9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_df(X_like, ref_df):\n",
    "#  convierte ndarray -> DataFrame con los mismos nombres.\n",
    "    if isinstance(X_like, np.ndarray):\n",
    "        return pd.DataFrame(X_like, columns=ref_df.columns)\n",
    "    return X_like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d3042b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metricas(y_true, y_pred):\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mape = np.mean(np.abs((y_true - y_pred)[y_true != 0] / y_true[y_true != 0])) * 100\n",
    "    return {\"MAE\": mae, \"RMSE\": rmse, \"MAPE\": mape}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "361815ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelo_con_gridsearch(modelo_base, param_grid,X_train, X_val, X_test, y_train, y_val, y_test, numeric_cols=None):\n",
    "    if numeric_cols is None:\n",
    "        numeric_cols = X_train.columns                \n",
    "    preproc = ColumnTransformer(\n",
    "        [(\"scale\", StandardScaler(), numeric_cols)],\n",
    "        remainder=\"drop\"\n",
    "    ).set_output(transform=\"pandas\")                  \n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        (\"pre\", preproc),\n",
    "        (\"modelo\", modelo_base)\n",
    "    ])\n",
    "\n",
    "    param_grid_pipeline = {f\"modelo__{k}\": v for k, v in param_grid.items()}\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    grid = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grid_pipeline,\n",
    "        cv=tscv,\n",
    "        scoring=\"neg_root_mean_squared_error\",\n",
    "        refit=True\n",
    "    )\n",
    "    grid.fit(X_train, y_train)\n",
    "    print(\"Mejores parámetros:\", grid.best_params_)\n",
    "\n",
    "    best_model = grid.best_estimator_          \n",
    "\n",
    "    # evaluación en validacion\n",
    "    y_pred_val = best_model.predict(to_df(X_val, X_train))\n",
    "    val_scores = metricas(y_val, y_pred_val)\n",
    "    print(f\"[VALIDACIÓN] MAE={val_scores['MAE']:.2f}, \"\n",
    "          f\"RMSE={val_scores['RMSE']:.2f}, MAPE={val_scores['MAPE']:.2f}%\")\n",
    "\n",
    "    # refit en train+val\n",
    "    X_train_val = pd.concat([X_train, X_val], axis=0)\n",
    "    y_train_val = pd.concat([y_train, y_val], axis=0)\n",
    "    best_model.fit(X_train_val, y_train_val)\n",
    "\n",
    "    # evaluación final en test\n",
    "    y_pred_test = best_model.predict(to_df(X_test, X_train))\n",
    "    test_scores = metricas(y_test, y_pred_test)\n",
    "    print(f\"[TEST] MAE={test_scores['MAE']:.2f}, \"\n",
    "          f\"RMSE={test_scores['RMSE']:.2f}, MAPE={test_scores['MAPE']:.2f}%\")\n",
    "\n",
    "    return best_model, {\"val\": val_scores, \"test\": test_scores}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c4a7430a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#regresion lineal\n",
    "def modelo_lineal(*args):\n",
    "    return modelo_con_gridsearch(LinearRegression(), {}, *args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "47b97867",
   "metadata": {},
   "outputs": [],
   "source": [
    "#estoacastico\n",
    "def modelo_estocastico(*args):\n",
    "    param_grid = {'alpha': [0.00001,0.0001, 0.001, 0.01,0.1,1,10,100], 'penalty': ['l2', 'elasticnet']}\n",
    "    return modelo_con_gridsearch(SGDRegressor(max_iter=1000, tol=1e-3,random_state=RANDOM_STATE), param_grid, *args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "1756242d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ridge\n",
    "def modelo_ridge(*args):\n",
    "    param_grid = {'alpha': [0.1, 1.0, 10.0]}\n",
    "    return modelo_con_gridsearch(Ridge(), param_grid, *args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "18ff99bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forest\n",
    "def modelo_rf(*args):\n",
    "    param_grid = {'n_estimators': [100, 200], 'max_depth': [3, 5, 10]}\n",
    "    return modelo_con_gridsearch(RandomForestRegressor(random_state=RANDOM_STATE), param_grid, *args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "178f5564",
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgboost\n",
    "def modelo_xgb(*args):\n",
    "    param_grid = {'n_estimators': [100], 'max_depth': [3, 5], 'learning_rate': [0.1]}\n",
    "    return modelo_con_gridsearch(XGBRegressor(verbosity=0,random_state=RANDOM_STATE), param_grid, *args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "df79d6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lightGBM\n",
    "def modelo_lgbm(*args):\n",
    "    param_grid = {'n_estimators': [100], 'max_depth': [3, 5], 'learning_rate': [0.1]}\n",
    "    #si algo se ve raro cambiar el verbose\n",
    "    return modelo_con_gridsearch(LGBMRegressor(verbose=-1,random_state=RANDOM_STATE), param_grid, *args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1ea9fa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#catboost\n",
    "def modelo_catboost(*args):\n",
    "    param_grid = {'iterations': [100], 'depth': [3, 5], 'learning_rate': [0.1]}\n",
    "    #si algo se ve raro cambiar el verbose\n",
    "    return modelo_con_gridsearch(CatBoostRegressor(verbose=0,random_state=RANDOM_STATE), param_grid, *args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "0a937a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.base import clone\n",
    "import pandas as pd\n",
    "\n",
    "def ensamblador_stacking_gridsearch(\n",
    "    base_estimators: dict,\n",
    "    param_grids: dict,\n",
    "    final_estimator,\n",
    "    X_train, X_val, X_test,\n",
    "    y_train, y_val, y_test,\n",
    "    numeric_cols=None\n",
    "):\n",
    "\n",
    "    best_models = []\n",
    "\n",
    "    # Grid search por modelo base\n",
    "    for name, est in base_estimators.items():\n",
    "        print(f\"\\n====== Optimizando {name} ======\")\n",
    "        best_model, _ = modelo_con_gridsearch(\n",
    "            modelo_base=est,\n",
    "            param_grid=param_grids[name],\n",
    "            X_train=X_train, X_val=X_val, X_test=X_test,\n",
    "            y_train=y_train, y_val=y_val, y_test=y_test,\n",
    "            numeric_cols=numeric_cols\n",
    "        )\n",
    "        # clone() devuelve una copia *sin entrenar* apta para StackingRegressor\n",
    "        best_models.append((name, clone(best_model)))\n",
    "\n",
    "    # definir y entrenar el stacking\n",
    "    stacker = StackingRegressor(\n",
    "        estimators=best_models,\n",
    "        final_estimator=final_estimator,\n",
    "        n_jobs=-1,\n",
    "        passthrough=False        # solo usamos las predicciones, no las X originales\n",
    "    )\n",
    "\n",
    "    # train + val retrain\n",
    "    X_train_val = pd.concat([X_train, X_val])\n",
    "    y_train_val = pd.concat([y_train, y_val])\n",
    "    stacker.fit(X_train_val, y_train_val)\n",
    "\n",
    "    #evaluacion\n",
    "    y_pred_val  = stacker.predict(to_df(X_val,  X_train))\n",
    "    val_scores  = metricas(y_val,  y_pred_val)\n",
    "    print(f\"[STACKING VALIDACIÓN] MAE={val_scores['MAE']:.2f}, \"\n",
    "          f\"RMSE={val_scores['RMSE']:.2f}, MAPE={val_scores['MAPE']:.2f}%\")\n",
    "\n",
    "    y_pred_test = stacker.predict(to_df(X_test, X_train))\n",
    "    test_scores = metricas(y_test, y_pred_test)\n",
    "    print(f\"[STACKING TEST]       MAE={test_scores['MAE']:.2f}, \"\n",
    "          f\"RMSE={test_scores['RMSE']:.2f}, MAPE={test_scores['MAPE']:.2f}%\")\n",
    "\n",
    "    return stacker, {\"val\": val_scores, \"test\": test_scores}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "879671ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelo_stacking(X_train, X_val, X_test,\n",
    "                    y_train, y_val, y_test,\n",
    "                    numeric_cols=None):\n",
    "\n",
    "    base_estimators = {\n",
    "        \"ridge\": Ridge(),\n",
    "        \"rf\"   : RandomForestRegressor(random_state=RANDOM_STATE),\n",
    "        \"xgb\"  : XGBRegressor(verbosity=0, random_state=RANDOM_STATE)\n",
    "    }\n",
    "    param_grids = {\n",
    "        \"ridge\": {\"alpha\": [0.1, 1.0, 10]},\n",
    "        \"rf\"   : {\"n_estimators\": [100, 200],\n",
    "                  \"max_depth\"  : [3, 5, 10]},\n",
    "        \"xgb\"  : {\"n_estimators\"  : [200, 400],\n",
    "                  \"learning_rate\": [0.05, 0.1]}\n",
    "    }\n",
    "\n",
    "    stack_model, scores = ensamblador_stacking_gridsearch(\n",
    "        base_estimators=base_estimators,\n",
    "        param_grids=param_grids,\n",
    "        final_estimator=LinearRegression(),\n",
    "        X_train=X_train, X_val=X_val, X_test=X_test,\n",
    "        y_train=y_train, y_val=y_val, y_test=y_test,\n",
    "        numeric_cols=numeric_cols\n",
    "    )\n",
    "\n",
    "    return stack_model, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d0544600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import clone\n",
    "from sklearn.linear_model import LinearRegression  # meta‑modelo por defecto\n",
    "\n",
    "def ensamblador_blending_gridsearch(\n",
    "    base_estimators: dict,\n",
    "    param_grids: dict,\n",
    "    final_estimator,\n",
    "    X_train, X_val, X_test,\n",
    "    y_train, y_val, y_test,\n",
    "    numeric_cols=None\n",
    "):\n",
    "\n",
    "    best_models = {}\n",
    "    val_meta_X  = [] \n",
    "    test_meta_X = []\n",
    "\n",
    "    # Grid search por modelo base\n",
    "    for name, est in base_estimators.items():\n",
    "        print(f\"\\n====== Optimizando {name} ======\")\n",
    "        best_model, _ = modelo_con_gridsearch(\n",
    "            modelo_base=est,\n",
    "            param_grid=param_grids[name],\n",
    "            X_train=X_train, X_val=X_val, X_test=X_test,\n",
    "            y_train=y_train, y_val=y_val, y_test=y_test,\n",
    "            numeric_cols=numeric_cols\n",
    "        )\n",
    "        best_models[name] = clone(best_model)\n",
    "\n",
    "    #entrenamiento\n",
    "    for name, mdl in best_models.items():\n",
    "        mdl.fit(to_df(X_train, X_train), y_train)          \n",
    "        val_meta_X.append(mdl.predict(to_df(X_val,  X_train)))\n",
    "    \n",
    "    val_meta_X = np.column_stack(val_meta_X)               \n",
    "\n",
    "    # entreno meta‑modelo con las preds de VAL\n",
    "    meta = clone(final_estimator)\n",
    "    meta.fit(val_meta_X, y_val)\n",
    "    \n",
    "    # train + val retrain\n",
    "    X_train_val = pd.concat([X_train, X_val])\n",
    "    y_train_val = pd.concat([y_train, y_val])\n",
    "\n",
    "    test_meta_X = []\n",
    "    for name, mdl in best_models.items():\n",
    "        mdl.fit(to_df(X_train_val, X_train), y_train_val)  # Train+Val\n",
    "        test_meta_X.append(mdl.predict(to_df(X_test, X_train)))\n",
    "\n",
    "    test_meta_X = np.column_stack(test_meta_X)\n",
    "    y_pred_test = meta.predict(test_meta_X)\n",
    "\n",
    "    # metricas\n",
    "    y_pred_val = meta.predict(val_meta_X)\n",
    "    val_scores  = metricas(y_val,  y_pred_val)\n",
    "    test_scores = metricas(y_test, y_pred_test)\n",
    "\n",
    "    print(f\"[BLENDING VALIDACIÓN] MAE={val_scores['MAE']:.2f}, \"\n",
    "          f\"RMSE={val_scores['RMSE']:.2f}, MAPE={val_scores['MAPE']:.2f}%\")\n",
    "    print(f\"[BLENDING TEST]       MAE={test_scores['MAE']:.2f}, \"\n",
    "          f\"RMSE={test_scores['RMSE']:.2f}, MAPE={test_scores['MAPE']:.2f}%\")\n",
    "\n",
    "    # **Asigna sus propias entradas** (los nombres de los base_estimators)\n",
    "    names = list(best_models.keys())  \n",
    "    # scikit-learn expects a numpy array here\n",
    "    meta.feature_names_in_ = np.array(names, dtype=object)\n",
    "\n",
    "    return meta, {\"val\": val_scores, \"test\": test_scores}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "1c5bb425",
   "metadata": {},
   "outputs": [],
   "source": [
    "#blending\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "def modelo_blending(X_train, X_val, X_test,\n",
    "                    y_train, y_val, y_test,\n",
    "                    numeric_cols=None):\n",
    "\n",
    "    base_estimators = {\n",
    "        \"ridge\": Ridge(),\n",
    "        \"rf\"   : RandomForestRegressor(random_state=RANDOM_STATE),\n",
    "        \"xgb\"  : XGBRegressor(verbosity=0, random_state=RANDOM_STATE)\n",
    "    }\n",
    "    param_grids = {\n",
    "        \"ridge\": {\"alpha\": [0.1, 1.0, 10]},\n",
    "        \"rf\"   : {\"n_estimators\": [100, 200],\n",
    "                  \"max_depth\"  : [3, 5, 10]},\n",
    "        \"xgb\"  : {\"n_estimators\"  : [200, 400],\n",
    "                  \"learning_rate\": [0.05, 0.1]}\n",
    "    }\n",
    "\n",
    "    blending_model, scores = ensamblador_blending_gridsearch(\n",
    "        base_estimators=base_estimators,\n",
    "        param_grids=param_grids,\n",
    "        final_estimator=LinearRegression(),\n",
    "        X_train=X_train, X_val=X_val, X_test=X_test,\n",
    "        y_train=y_train, y_val=y_val, y_test=y_test,\n",
    "        numeric_cols=numeric_cols\n",
    "    )\n",
    "    return blending_model, scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "985b909a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ejecutar_modelos(df, target_col, fecha_col='fecha_primera_sesion'):\n",
    "    # Definimos las columnas a excluir del modelado\n",
    "    columnas_excluir = [\n",
    "        target_col,\n",
    "        fecha_col,\n",
    "        'uid',\n",
    "        'fecha_primera_compra',\n",
    "        'fecha_ultima_compra',\n",
    "        'fecha_primera_sesion',\n",
    "        'primer_source'\n",
    "    ]\n",
    "\n",
    "    # Limpiamos nulos\n",
    "    df = df.dropna(subset=[target_col]).dropna()\n",
    "\n",
    "    # Particionamos\n",
    "    train, val, test = dividir_dataset(df, fecha_col)\n",
    "\n",
    "    # X / y\n",
    "    X_train, y_train = train.drop(columns=columnas_excluir), train[target_col]\n",
    "    X_val,   y_val   = val.drop(columns=columnas_excluir),   val[target_col]\n",
    "    X_test,  y_test  = test.drop(columns=columnas_excluir),  test[target_col]\n",
    "\n",
    "    modelos = {}\n",
    "    evaluaciones = {}\n",
    "\n",
    "    print(\"Modelos de regresión:\")\n",
    "    print('\\nLineal\\n')\n",
    "    modelos['lineal'],      evaluaciones['lineal']      = modelo_lineal(X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "    print('\\nEstocástico\\n')\n",
    "    modelos['estocastico'], evaluaciones['estocastico'] = modelo_estocastico(X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "    print('\\nRidge\\n')\n",
    "    modelos['ridge'],       evaluaciones['ridge']       = modelo_ridge(X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "\n",
    "    print(\"\\n\\nModelos avanzados:\")\n",
    "    print('\\nRandom Forest\\n')\n",
    "    modelos['rf'],          evaluaciones['rf']          = modelo_rf(X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "    print('\\nXGBoost\\n')\n",
    "    modelos['xgb'],         evaluaciones['xgb']         = modelo_xgb(X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "    print('\\nLightGBM\\n')\n",
    "    modelos['lgbm'],        evaluaciones['lgbm']        = modelo_lgbm(X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "    print('\\nCatBoost\\n')\n",
    "    modelos['catboost'],    evaluaciones['catboost']    = modelo_catboost(X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "\n",
    "    print(\"\\n\\nModelos ensambladores:\")\n",
    "    print('\\nStacking\\n')\n",
    "    modelos['stacking'],    evaluaciones['stacking']    = modelo_stacking(X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "    print('\\nBlending\\n')\n",
    "    modelos['blending'],    evaluaciones['blending']    = modelo_blending(X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "\n",
    "    # ——— Nuevo bloque: elegir el mejor por MAE en validación ———\n",
    "    sample = next(iter(evaluaciones))\n",
    "    keys = set(evaluaciones[sample].keys())\n",
    "\n",
    "    # 2) Definimos cómo extraer el MAE de validación\n",
    "    if 'validation' in keys:\n",
    "        # caso anidado: evaluaciones[m]['validation']['mae']\n",
    "        get_mae = lambda m: evaluaciones[m]['validation']['mae']\n",
    "    elif 'val' in keys and 'MAE' in evaluaciones[sample]['val']:\n",
    "        # caso pipeline con llave 'val' y dentro 'MAE'\n",
    "        get_mae = lambda m: evaluaciones[m]['val']['MAE']\n",
    "    elif 'MAE' in keys:\n",
    "        # caso plano: evaluaciones[m]['MAE']\n",
    "        get_mae = lambda m: evaluaciones[m]['MAE']\n",
    "    else:\n",
    "        raise KeyError(f\"No encontré MAE en las keys: {keys}\")\n",
    "\n",
    "    # 3) Selección del mejor\n",
    "    best_model = min(evaluaciones.keys(), key=get_mae)\n",
    "    # 4) Recuperamos sus métricas\n",
    "    best = evaluaciones[best_model]\n",
    "    # Si es anidado o usa 'val', nivelamos a un dict plano\n",
    "    if 'validation' in keys:\n",
    "        best_val = best['validation']\n",
    "    elif 'val' in keys:\n",
    "        best_val = best['val']\n",
    "    else:\n",
    "        best_val = {'MAE': best['MAE'], 'RMSE': best.get('RMSE'), 'MAPE': best.get('MAPE')}\n",
    "\n",
    "    print(f\"\\n>>> Mejor modelo en VALIDACIÓN: {best_model.upper()}\")\n",
    "    print(f\"    MAE  = {best_val['MAE']:.2f}\")\n",
    "    print(f\"    RMSE = {best_val['RMSE']:.2f}\")\n",
    "    print(f\"    MAPE = {best_val['MAPE']:.2f}%\")\n",
    "\n",
    "    return modelos, evaluaciones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9655aff",
   "metadata": {},
   "source": [
    "# Importación de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "8fd83b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "def guardar_modelos_y_resultados(\n",
    "    modelos,\n",
    "    evaluaciones,\n",
    "    target_name,\n",
    "    features_dict=None   # ahora opcional\n",
    "):\n",
    "    \"\"\"\n",
    "    modelos:       dict {nombre_modelo: objeto_modelo}\n",
    "    evaluaciones:  dict de métricas (sin usar aquí, pero por consistencia)\n",
    "    target_name:   'LTV_180' o 'CAC_source_30'\n",
    "    features_dict: opcional dict {nombre_modelo: [lista_de_columnas_usadas]}\n",
    "    \"\"\"\n",
    "    models_dir = f\"../models/{target_name}\"\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "    for nombre, modelo in modelos.items():\n",
    "        # 1) Guardar el pickle del modelo\n",
    "        ruta_modelo = os.path.join(models_dir, f\"{nombre}.pkl\")\n",
    "        with open(ruta_modelo, \"wb\") as f:\n",
    "            pickle.dump(modelo, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        # 2) Averiguar lista de features\n",
    "        feats = None\n",
    "\n",
    "        # a) si me la pasan explícita en features_dict\n",
    "        if features_dict and nombre in features_dict:\n",
    "            feats = features_dict[nombre]\n",
    "\n",
    "        # b) si el modelo tiene feature_names_in_ (sklearn o XGBoost/LGBM)\n",
    "        elif hasattr(modelo, \"feature_names_in_\"):\n",
    "            feats = modelo.feature_names_in_.tolist()\n",
    "\n",
    "        # 3) Guardar lista de features en JSON si la encontramos\n",
    "        if feats is not None:\n",
    "            ruta_feats = os.path.join(models_dir, f\"{nombre}_features.json\")\n",
    "            with open(ruta_feats, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(feats, f, ensure_ascii=False, indent=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f5159582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostrar_resultados(evaluaciones, target_name):\n",
    "    resultados_df = pd.DataFrame({\n",
    "        modelo: scores[\"test\"] for modelo, scores in evaluaciones.items()\n",
    "    }).T\n",
    "    resultados_df.index.name = f\"Modelos ({target_name})\"\n",
    "    display(resultados_df.sort_values('MAPE'))\n",
    "    return resultados_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f424968d",
   "metadata": {},
   "source": [
    "### Implementación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "0a479c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ENTRENAMIENTO Y EVALUACION DE LTV\n",
      "\n",
      "\n",
      "Modelos de regresión:\n",
      "\n",
      "Lineal\n",
      "\n",
      "Mejores parámetros: {}\n",
      "[VALIDACIÓN] MAE=3255361398621.24, RMSE=3255361398621.24, MAPE=88334845598783.39%\n",
      "[TEST] MAE=5.47, RMSE=9.25, MAPE=106.13%\n",
      "\n",
      "Estocástico\n",
      "\n",
      "Mejores parámetros: {'modelo__alpha': 0.1, 'modelo__penalty': 'l2'}\n",
      "[VALIDACIÓN] MAE=44.55, RMSE=63.99, MAPE=1278.18%\n",
      "[TEST] MAE=14.50, RMSE=21.52, MAPE=477.25%\n",
      "\n",
      "Ridge\n",
      "\n",
      "Mejores parámetros: {'modelo__alpha': 0.1}\n",
      "[VALIDACIÓN] MAE=6.85, RMSE=16.13, MAPE=132.01%\n",
      "[TEST] MAE=5.48, RMSE=9.25, MAPE=106.42%\n",
      "\n",
      "\n",
      "Modelos avanzados:\n",
      "\n",
      "Random Forest\n",
      "\n",
      "Mejores parámetros: {'modelo__max_depth': 10, 'modelo__n_estimators': 100}\n",
      "[VALIDACIÓN] MAE=0.74, RMSE=6.86, MAPE=2.60%\n",
      "[TEST] MAE=1.36, RMSE=14.48, MAPE=2.17%\n",
      "\n",
      "XGBoost\n",
      "\n",
      "Mejores parámetros: {'modelo__learning_rate': 0.1, 'modelo__max_depth': 5, 'modelo__n_estimators': 100}\n",
      "[VALIDACIÓN] MAE=1.09, RMSE=5.95, MAPE=15.59%\n",
      "[TEST] MAE=1.38, RMSE=6.27, MAPE=24.80%\n",
      "\n",
      "LightGBM\n",
      "\n",
      "Mejores parámetros: {'modelo__learning_rate': 0.1, 'modelo__max_depth': 3, 'modelo__n_estimators': 100}\n",
      "[VALIDACIÓN] MAE=8.47, RMSE=36.86, MAPE=71.15%\n",
      "[TEST] MAE=5.82, RMSE=15.08, MAPE=102.57%\n",
      "\n",
      "CatBoost\n",
      "\n",
      "Mejores parámetros: {'modelo__depth': 3, 'modelo__iterations': 100, 'modelo__learning_rate': 0.1}\n",
      "[VALIDACIÓN] MAE=5.78, RMSE=16.99, MAPE=97.12%\n",
      "[TEST] MAE=18.69, RMSE=177.97, MAPE=84.47%\n",
      "\n",
      "\n",
      "Modelos ensambladores:\n",
      "\n",
      "Stacking\n",
      "\n",
      "\n",
      "====== Optimizando ridge ======\n",
      "Mejores parámetros: {'modelo__alpha': 0.1}\n",
      "[VALIDACIÓN] MAE=6.85, RMSE=16.13, MAPE=132.01%\n",
      "[TEST] MAE=5.48, RMSE=9.25, MAPE=106.42%\n",
      "\n",
      "====== Optimizando rf ======\n",
      "Mejores parámetros: {'modelo__max_depth': 10, 'modelo__n_estimators': 100}\n",
      "[VALIDACIÓN] MAE=0.74, RMSE=6.86, MAPE=2.60%\n",
      "[TEST] MAE=1.36, RMSE=14.48, MAPE=2.17%\n",
      "\n",
      "====== Optimizando xgb ======\n",
      "Mejores parámetros: {'modelo__learning_rate': 0.1, 'modelo__n_estimators': 400}\n",
      "[VALIDACIÓN] MAE=0.95, RMSE=8.66, MAPE=2.97%\n",
      "[TEST] MAE=0.52, RMSE=2.80, MAPE=2.37%\n",
      "[STACKING VALIDACIÓN] MAE=3.39, RMSE=7.95, MAPE=48.88%\n",
      "[STACKING TEST]       MAE=3.63, RMSE=5.38, MAPE=58.02%\n",
      "\n",
      "Blending\n",
      "\n",
      "\n",
      "====== Optimizando ridge ======\n",
      "Mejores parámetros: {'modelo__alpha': 0.1}\n",
      "[VALIDACIÓN] MAE=6.85, RMSE=16.13, MAPE=132.01%\n",
      "[TEST] MAE=5.48, RMSE=9.25, MAPE=106.42%\n",
      "\n",
      "====== Optimizando rf ======\n",
      "Mejores parámetros: {'modelo__max_depth': 10, 'modelo__n_estimators': 100}\n",
      "[VALIDACIÓN] MAE=0.74, RMSE=6.86, MAPE=2.60%\n",
      "[TEST] MAE=1.36, RMSE=14.48, MAPE=2.17%\n",
      "\n",
      "====== Optimizando xgb ======\n",
      "Mejores parámetros: {'modelo__learning_rate': 0.1, 'modelo__n_estimators': 400}\n",
      "[VALIDACIÓN] MAE=0.95, RMSE=8.66, MAPE=2.97%\n",
      "[TEST] MAE=0.52, RMSE=2.80, MAPE=2.37%\n",
      "[BLENDING VALIDACIÓN] MAE=0.64, RMSE=3.26, MAPE=5.11%\n",
      "[BLENDING TEST]       MAE=1.42, RMSE=8.12, MAPE=20.49%\n",
      "\n",
      ">>> Mejor modelo en VALIDACIÓN: BLENDING\n",
      "    MAE  = 0.64\n",
      "    RMSE = 3.26\n",
      "    MAPE = 5.11%\n",
      "\n",
      "\n",
      "ENTRENAMIENTO Y EVALUACION DE CAC\n",
      "\n",
      "\n",
      "Modelos de regresión:\n",
      "\n",
      "Lineal\n",
      "\n",
      "Mejores parámetros: {}\n",
      "[VALIDACIÓN] MAE=0.13, RMSE=0.15, MAPE=42.48%\n",
      "[TEST] MAE=0.13, RMSE=0.15, MAPE=43.13%\n",
      "\n",
      "Estocástico\n",
      "\n",
      "Mejores parámetros: {'modelo__alpha': 0.1, 'modelo__penalty': 'elasticnet'}\n",
      "[VALIDACIÓN] MAE=0.13, RMSE=0.15, MAPE=45.03%\n",
      "[TEST] MAE=0.13, RMSE=0.15, MAPE=43.03%\n",
      "\n",
      "Ridge\n",
      "\n",
      "Mejores parámetros: {'modelo__alpha': 10.0}\n",
      "[VALIDACIÓN] MAE=0.13, RMSE=0.15, MAPE=42.43%\n",
      "[TEST] MAE=0.13, RMSE=0.15, MAPE=43.44%\n",
      "\n",
      "\n",
      "Modelos avanzados:\n",
      "\n",
      "Random Forest\n",
      "\n",
      "Mejores parámetros: {'modelo__max_depth': 3, 'modelo__n_estimators': 200}\n",
      "[VALIDACIÓN] MAE=0.13, RMSE=0.15, MAPE=46.19%\n",
      "[TEST] MAE=0.13, RMSE=0.15, MAPE=43.20%\n",
      "\n",
      "XGBoost\n",
      "\n",
      "Mejores parámetros: {'modelo__learning_rate': 0.1, 'modelo__max_depth': 3, 'modelo__n_estimators': 100}\n",
      "[VALIDACIÓN] MAE=0.13, RMSE=0.16, MAPE=45.30%\n",
      "[TEST] MAE=0.13, RMSE=0.15, MAPE=42.14%\n",
      "\n",
      "LightGBM\n",
      "\n",
      "Mejores parámetros: {'modelo__learning_rate': 0.1, 'modelo__max_depth': 3, 'modelo__n_estimators': 100}\n",
      "[VALIDACIÓN] MAE=0.13, RMSE=0.16, MAPE=44.94%\n",
      "[TEST] MAE=0.13, RMSE=0.15, MAPE=42.28%\n",
      "\n",
      "CatBoost\n",
      "\n",
      "Mejores parámetros: {'modelo__depth': 3, 'modelo__iterations': 100, 'modelo__learning_rate': 0.1}\n",
      "[VALIDACIÓN] MAE=0.13, RMSE=0.15, MAPE=45.05%\n",
      "[TEST] MAE=0.13, RMSE=0.15, MAPE=42.44%\n",
      "\n",
      "\n",
      "Modelos ensambladores:\n",
      "\n",
      "Stacking\n",
      "\n",
      "\n",
      "====== Optimizando ridge ======\n",
      "Mejores parámetros: {'modelo__alpha': 10}\n",
      "[VALIDACIÓN] MAE=0.13, RMSE=0.15, MAPE=42.43%\n",
      "[TEST] MAE=0.13, RMSE=0.15, MAPE=43.44%\n",
      "\n",
      "====== Optimizando rf ======\n",
      "Mejores parámetros: {'modelo__max_depth': 3, 'modelo__n_estimators': 200}\n",
      "[VALIDACIÓN] MAE=0.13, RMSE=0.15, MAPE=46.19%\n",
      "[TEST] MAE=0.13, RMSE=0.15, MAPE=43.20%\n",
      "\n",
      "====== Optimizando xgb ======\n",
      "Mejores parámetros: {'modelo__learning_rate': 0.05, 'modelo__n_estimators': 200}\n",
      "[VALIDACIÓN] MAE=0.13, RMSE=0.16, MAPE=45.68%\n",
      "[TEST] MAE=0.13, RMSE=0.15, MAPE=42.76%\n",
      "[STACKING VALIDACIÓN] MAE=0.13, RMSE=0.16, MAPE=47.22%\n",
      "[STACKING TEST]       MAE=0.13, RMSE=0.15, MAPE=43.27%\n",
      "\n",
      "Blending\n",
      "\n",
      "\n",
      "====== Optimizando ridge ======\n",
      "Mejores parámetros: {'modelo__alpha': 10}\n",
      "[VALIDACIÓN] MAE=0.13, RMSE=0.15, MAPE=42.43%\n",
      "[TEST] MAE=0.13, RMSE=0.15, MAPE=43.44%\n",
      "\n",
      "====== Optimizando rf ======\n",
      "Mejores parámetros: {'modelo__max_depth': 3, 'modelo__n_estimators': 200}\n",
      "[VALIDACIÓN] MAE=0.13, RMSE=0.15, MAPE=46.19%\n",
      "[TEST] MAE=0.13, RMSE=0.15, MAPE=43.20%\n",
      "\n",
      "====== Optimizando xgb ======\n",
      "Mejores parámetros: {'modelo__learning_rate': 0.05, 'modelo__n_estimators': 200}\n",
      "[VALIDACIÓN] MAE=0.13, RMSE=0.16, MAPE=45.68%\n",
      "[TEST] MAE=0.13, RMSE=0.15, MAPE=42.76%\n",
      "[BLENDING VALIDACIÓN] MAE=0.13, RMSE=0.15, MAPE=46.11%\n",
      "[BLENDING TEST]       MAE=0.13, RMSE=0.15, MAPE=45.61%\n",
      "\n",
      ">>> Mejor modelo en VALIDACIÓN: RIDGE\n",
      "    MAE  = 0.13\n",
      "    RMSE = 0.15\n",
      "    MAPE = 42.43%\n"
     ]
    }
   ],
   "source": [
    "# Entrenar y guardar LTV\n",
    "print(\"\\n\\nENTRENAMIENTO Y EVALUACION DE LTV\\n\\n\")\n",
    "modelos_ltv, evaluaciones_ltv = ejecutar_modelos(df, target_col='LTV_180')\n",
    "guardar_modelos_y_resultados(modelos_ltv, evaluaciones_ltv, target_name='LTV_180')\n",
    "\n",
    "# Entrenar y guardar CAC\n",
    "print(\"\\n\\nENTRENAMIENTO Y EVALUACION DE CAC\\n\\n\")\n",
    "modelos_cac, evaluaciones_cac = ejecutar_modelos(df, target_col='CAC_source_30')\n",
    "guardar_modelos_y_resultados(modelos_cac, evaluaciones_cac, target_name='CAC_source_30')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "44a93801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Baseline (mean) para LTV_180 ---\n",
      "[VAL]  MAE=5.34, RMSE=13.80, MAPE=380.22%\n",
      "[TEST] MAE=5.56, RMSE=13.81, MAPE=430.84%\n",
      "\n",
      "--- Baseline (median) para LTV_180 ---\n",
      "[VAL]  MAE=3.27, RMSE=13.65, MAPE=134.28%\n",
      "[TEST] MAE=3.52, RMSE=13.68, MAPE=155.10%\n",
      "\n",
      "--- Baseline (mean) para CAC_source_30 ---\n",
      "[VAL]  MAE=0.13, RMSE=0.15, MAPE=45.41%\n",
      "[TEST] MAE=0.13, RMSE=0.15, MAPE=42.56%\n",
      "\n",
      "--- Baseline (median) para CAC_source_30 ---\n",
      "[VAL]  MAE=0.12, RMSE=0.17, MAPE=29.85%\n",
      "[TEST] MAE=0.11, RMSE=0.17, MAPE=28.29%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "def eval_baseline(df, target_col, fecha_col='fecha_primera_sesion'):\n",
    "    # 1) Particiona\n",
    "    train, val, test = dividir_dataset(df, fecha_col)\n",
    "\n",
    "    for strategy in ['mean','median']:\n",
    "        # 2) Calcula el valor fijo de predicción\n",
    "        y_train = train[target_col]\n",
    "        if strategy=='mean':\n",
    "            pred_value = y_train.mean()\n",
    "        else:\n",
    "            pred_value = y_train.median()\n",
    "\n",
    "        # 3) Genera vectores de predicción\n",
    "        y_val   = val[target_col].values\n",
    "        y_test  = test[target_col].values\n",
    "        y_pred_val  = np.full_like(y_val,  pred_value, dtype=float)\n",
    "        y_pred_test = np.full_like(y_test, pred_value, dtype=float)\n",
    "\n",
    "        # 4) Calcula métricas\n",
    "        mae_val  = mean_absolute_error(y_val,  y_pred_val)\n",
    "        rmse_val = mean_squared_error(y_val,  y_pred_val, squared=False)\n",
    "        mape_val = np.mean(np.abs((y_val - y_pred_val)[y_val!=0] / y_val[y_val!=0]))*100\n",
    "\n",
    "        mae_test  = mean_absolute_error(y_test,  y_pred_test)\n",
    "        rmse_test = mean_squared_error(y_test,  y_pred_test, squared=False)\n",
    "        mape_test = np.mean(np.abs((y_test - y_pred_test)[y_test!=0] / y_test[y_test!=0]))*100\n",
    "\n",
    "        # 5) Imprime\n",
    "        print(f\"\\n--- Baseline ({strategy}) para {target_col} ---\")\n",
    "        print(f\"[VAL]  MAE={mae_val:.2f}, RMSE={rmse_val:.2f}, MAPE={mape_val:.2f}%\")\n",
    "        print(f\"[TEST] MAE={mae_test:.2f}, RMSE={rmse_test:.2f}, MAPE={mape_test:.2f}%\")\n",
    "\n",
    "# Ejecútalo así:\n",
    "eval_baseline(df, target_col='LTV_180')\n",
    "eval_baseline(df, target_col='CAC_source_30')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "82e48a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumen de las métricas de LTV\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAPE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Modelos (LTV_180)</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>1.359281</td>\n",
       "      <td>14.478625</td>\n",
       "      <td>2.165180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blending</th>\n",
       "      <td>1.421337</td>\n",
       "      <td>8.118607</td>\n",
       "      <td>20.494380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>1.380891</td>\n",
       "      <td>6.273534</td>\n",
       "      <td>24.795161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stacking</th>\n",
       "      <td>3.630236</td>\n",
       "      <td>5.383634</td>\n",
       "      <td>58.023771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>catboost</th>\n",
       "      <td>18.693288</td>\n",
       "      <td>177.972063</td>\n",
       "      <td>84.472639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lgbm</th>\n",
       "      <td>5.824010</td>\n",
       "      <td>15.079647</td>\n",
       "      <td>102.567909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lineal</th>\n",
       "      <td>5.473496</td>\n",
       "      <td>9.252063</td>\n",
       "      <td>106.132157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>5.482115</td>\n",
       "      <td>9.252748</td>\n",
       "      <td>106.417487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>estocastico</th>\n",
       "      <td>14.496829</td>\n",
       "      <td>21.517275</td>\n",
       "      <td>477.250075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         MAE        RMSE        MAPE\n",
       "Modelos (LTV_180)                                   \n",
       "rf                  1.359281   14.478625    2.165180\n",
       "blending            1.421337    8.118607   20.494380\n",
       "xgb                 1.380891    6.273534   24.795161\n",
       "stacking            3.630236    5.383634   58.023771\n",
       "catboost           18.693288  177.972063   84.472639\n",
       "lgbm                5.824010   15.079647  102.567909\n",
       "lineal              5.473496    9.252063  106.132157\n",
       "ridge               5.482115    9.252748  106.417487\n",
       "estocastico        14.496829   21.517275  477.250075"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resumen de las métricas de CAC\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAPE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Modelos (CAC_source_30)</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>0.125745</td>\n",
       "      <td>0.149223</td>\n",
       "      <td>42.142213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lgbm</th>\n",
       "      <td>0.125865</td>\n",
       "      <td>0.149211</td>\n",
       "      <td>42.283318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>catboost</th>\n",
       "      <td>0.126794</td>\n",
       "      <td>0.150225</td>\n",
       "      <td>42.444528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>estocastico</th>\n",
       "      <td>0.127560</td>\n",
       "      <td>0.150329</td>\n",
       "      <td>43.028965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lineal</th>\n",
       "      <td>0.127554</td>\n",
       "      <td>0.149881</td>\n",
       "      <td>43.128816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>0.128037</td>\n",
       "      <td>0.150661</td>\n",
       "      <td>43.200952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stacking</th>\n",
       "      <td>0.127981</td>\n",
       "      <td>0.150269</td>\n",
       "      <td>43.266239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>0.128066</td>\n",
       "      <td>0.150006</td>\n",
       "      <td>43.435056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blending</th>\n",
       "      <td>0.131320</td>\n",
       "      <td>0.150454</td>\n",
       "      <td>45.607072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              MAE      RMSE       MAPE\n",
       "Modelos (CAC_source_30)                               \n",
       "xgb                      0.125745  0.149223  42.142213\n",
       "lgbm                     0.125865  0.149211  42.283318\n",
       "catboost                 0.126794  0.150225  42.444528\n",
       "estocastico              0.127560  0.150329  43.028965\n",
       "lineal                   0.127554  0.149881  43.128816\n",
       "rf                       0.128037  0.150661  43.200952\n",
       "stacking                 0.127981  0.150269  43.266239\n",
       "ridge                    0.128066  0.150006  43.435056\n",
       "blending                 0.131320  0.150454  45.607072"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Para LTV\n",
    "print('Resumen de las métricas de LTV')\n",
    "res_ltv = mostrar_resultados(evaluaciones_ltv, \"LTV_180\")\n",
    "\n",
    "# Para CAC\n",
    "print('\\nResumen de las métricas de CAC')\n",
    "res_cac = mostrar_resultados(evaluaciones_cac, \"CAC_source_30\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "24a6c8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Estadísticas de 'LTV_180' ==\n",
      "  Count     = 35636\n",
      "  Mean (μ)  = 6.42\n",
      "  Median    = 3.00\n",
      "  Std (σ)   = 85.57\n",
      "== Umbrales de métrica ==\n",
      "  MAE  ≤ 1.28    (20% de μ)\n",
      "  RMSE ≤ 1.60    (25% de μ)\n",
      "  MAPE < 30%\n",
      "\n",
      "== Estadísticas de 'CAC_source_30' ==\n",
      "  Count     = 36522\n",
      "  Mean (μ)  = 0.34\n",
      "  Median    = 0.26\n",
      "  Std (σ)   = 0.15\n",
      "== Umbrales de métrica ==\n",
      "  MAE  ≤ 0.07    (20% de μ)\n",
      "  RMSE ≤ 0.08    (25% de μ)\n",
      "  MAPE < 30%\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cálculo de umbrales esperados para tus métricas\n",
    "# =============================================================================\n",
    "def imprimir_thresholds(df, target_cols):\n",
    "    for target in target_cols:\n",
    "        y = df[target].dropna()\n",
    "        mu, med, sd = y.mean(), y.median(), y.std()\n",
    "        mae_thr  = 0.20 * mu\n",
    "        rmse_thr = 0.25 * mu\n",
    "        mape_thr = 30.0  # en porcentaje\n",
    "        \n",
    "        print(f\"\\n== Estadísticas de '{target}' ==\")\n",
    "        print(f\"  Count     = {len(y)}\")\n",
    "        print(f\"  Mean (μ)  = {mu:,.2f}\")\n",
    "        print(f\"  Median    = {med:,.2f}\")\n",
    "        print(f\"  Std (σ)   = {sd:,.2f}\")\n",
    "        print(\"== Umbrales de métrica ==\")\n",
    "        print(f\"  MAE  ≤ {mae_thr:,.2f}    ({mae_thr/mu:.0%} de μ)\")\n",
    "        print(f\"  RMSE ≤ {rmse_thr:,.2f}    ({rmse_thr/mu:.0%} de μ)\")\n",
    "        print(f\"  MAPE < {mape_thr:.0f}%\")\n",
    "        \n",
    "# Usar sobre tu df cargado:\n",
    "imprimir_thresholds(\n",
    "    df,\n",
    "    target_cols=['LTV_180','CAC_source_30']\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
